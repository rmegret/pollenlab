{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workshop: Pollen recognition using Convolutional Neuronal Networks. First steps in keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a model\n",
    "\n",
    "1. Debug here. \n",
    "2. When is ready move it to a python script. \n",
    "\n",
    "### Sequential \n",
    "\n",
    "During this workshop we will study only sequential models.There are other type of models that support more complicated and advanced architectures. \n",
    "\n",
    "The steps to create sequential models is very simple: \n",
    "\n",
    "*  Initialize your model by calling the class model Sequential()\n",
    "*  Add layers depending the functionality you want. \n",
    "    * Dense layers\n",
    "    * Convolutional Layer\n",
    "    * MaxPooling \n",
    "    * Flatten \n",
    "*  Make sure input dimensions are correct. \n",
    "    * Input dimensions come from the size of the input images. For the convolutional layer you need to specify\n",
    "    the dimensions and the channels in an specific order. Using theano backend \"channels first\". Using tensorflow backend \"channels last\"\n",
    "    * Check also the output. For binary classification usually the sumarizing layer only would have 2 units. \n",
    "    \n",
    "*  Compile your model\n",
    "\n",
    "*  Fit and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Convolution3D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def logistic_regresor(units=1,input_dim=32*32*3,\n",
    "\t\t\t\t\t\t activation='sigmoid', loss='binary_crossentropy',\n",
    "\t\t\t\t\t\t \t\t\t\toptimizer='sgd',metrics = 'accuracy'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(units, input_dim=input_dim, activation=activation))\n",
    "\n",
    "\tmodel.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[metrics])\n",
    "\treturn model \n",
    "\n",
    "def shallow_model(input_shape=(3,90,150), lr =0.001, kernels=16, stride=(5,5),pool_size=(2,2), dense=50):\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Conv2D(kernels, stride, input_shape=input_shape,data_format=\"channels_first\"))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(dense, activation='relu'))\n",
    "\tmodel.add(Dense(2, activation='softmax'))\n",
    "\tmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=lr),\n",
    "              metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "def two_layer_model(input_shape=(3,100,100), lr =0.001, kernels=16, stride=(13,13),pool_size=(2,2), dense=50):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(kernels, stride, input_shape=input_shape))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=pool_size))\n",
    "\tmodel.add(Conv2D(kernels, stride, input_shape=input_shape))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=pool_size))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(dense, activation='relu'))\n",
    "\tmodel.add(Dense(2, activation='softmax'))\n",
    "\tmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=lr),\n",
    "              metrics=['accuracy'])\n",
    "\treturn model\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkpoints  \n",
    "\n",
    "Training Neuronal Networks  can take long and might envolve some errors or issues on the way. Is always a good practice, for instance, save the weights often, save the logs according to the metrics used, and stop early if the training is not improving learning.  Gladly for us,  Keras make it very simple!  The keras  callbacks are design to call out functions with some periodicity to check how the network is doing. Let's check some of them\n",
    "\n",
    "* **ModelCheckpoint**: \n",
    "\n",
    "This call backs let us save the weights in some specific location, according to the periodicity we want or the metric we are using. \n",
    "\n",
    "* **CSVLogger**: We can also save all the logs obtained in an indicated location in csv format. \n",
    "\n",
    "* **TensorBoard**: This tool creates also logs, but it also includes an interface for visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "\n",
    "WEIGHTS_BEST = 'weights/weights.best.h5'\n",
    "TRAINING_LOG = 'logs/training_log,csv'\n",
    "LOGS_DIR = './logs'\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(WEIGHTS_BEST, monitor='loss', verbose=0, save_best_only=False, save_weights_only=True, mode='min', period=1) \n",
    "csv_logger = CSVLogger(TRAINING_LOG, append=True)\n",
    "tb = TensorBoard(log_dir=LOGS_DIR, histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "\n",
    "callbacks_list = [ checkpoint, csv_logger, tb]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your model, you can star training. To do so you can call the model.fit(*parameters*) method. It has the following inputs\n",
    "\n",
    "**Data, target **: Data for training and labels. More complicated models not only accept labels but could accept ground_truth images, such as heatmaps. Also this can be replaced by a generator that can be used for augmenting data. \n",
    "\n",
    "**batch Size:** How many images are you feeding on each step.\n",
    "\n",
    "**epochs:** The maximum number of epocs that you want to train your model. \n",
    "\n",
    "**validation data:** You might want to save the performance of your model while is training, so you can put your validation data to check the performance in your testing dataset. \n",
    "\n",
    "**callbacks:** see previous section\n",
    "\n",
    "After you finish training you can evaluate your model by providing testing dataset. You can specify what kind of metrics you can use such as F-1 score, accuracy, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regressor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "batch_size = 6\n",
    "max_iter= 200 \n",
    "model=logistic_regresor()\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs=max_iter,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=callbacks_list)\n",
    "results = model.evaluate(self.test,self.test_y)\n",
    "H_logistic = pd.DataFrame(history.history, index=history.epoch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One layer Shallow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "batch_size = 6\n",
    "max_iter= 200 \n",
    "model=shallow_model()\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs=max_iter,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=callbacks_list)\n",
    "results = model.evaluate(self.test,self.test_y)\n",
    "H_logistic = pd.DataFrame(history.history, index=history.epoch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Layer Shallow Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "batch_size = 6\n",
    "max_iter= 200 \n",
    "model=two_layer_model()\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs=max_iter,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=callbacks_list)\n",
    "results = model.evaluate(self.test,self.test_y)\n",
    "H_logistic = pd.DataFrame(history.history, index=history.epoch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results is a crucial to being able to say how good a model is working. \n",
    "There are basically two visualizations we will try to look at today.  \n",
    "\n",
    "1. Visualize the metrics, losses, logs... \n",
    "\n",
    "2. Visualize the confusion matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the metrics we can use the help of pandas and matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writting the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to write the scripts! Now that we know that the models are working and that input dimensions and architecture of the networks are compatible, we will put the different parts of the pipeline in python scripts and then we will create a bash script to run in Bridges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to make clear some important facts for running jobs on bridges. For future interaction with the server, please read carefully the [documentation](https://portal.xsede.org/psc-bridges).  Every job on bridges has some cost, so it is important to make sure to put the right options on the scripts so we don't waste resources. The pricing is based on the cores used. For this workshop we will used gpu-shared space. Using gpu-shared space is cheaper and usually these jobs get allocated faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start an interactive session type the following command: \n",
    "\n",
    "```bash\n",
    "interact -gpu \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
