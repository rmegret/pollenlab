{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workshop: Pollen recognition using Convolutional Neuronal Networks. First steps in keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a model\n",
    "\n",
    "1. Debug here. \n",
    "2. When is ready move it to a python script. \n",
    "\n",
    "### Sequential \n",
    "\n",
    "During this workshop we will study only sequential models.There are other type of models that support more complicated and advanced architectures. \n",
    "\n",
    "The steps to create sequential models is very simple: \n",
    "\n",
    "*  Initialize your model by calling the class model Sequential()\n",
    "*  Add layers depending the functionality you want. \n",
    "    * Dense layers\n",
    "    * Convolutional Layer\n",
    "    * MaxPooling \n",
    "    * Flatten \n",
    "*  Make sure input dimensions are correct. \n",
    "    * Input dimensions come from the size of the input images. For the convolutional layer you need to specify\n",
    "    the dimensions and the channels in an specific order. Using theano backend \"channels first\". Using tensorflow backend \"channels last\"\n",
    "    * Check also the output. For binary classification usually the sumarizing layer only would have 2 units. \n",
    "    \n",
    "*  Compile your model\n",
    "\n",
    "*  Fit and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a73ec2810ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Convolution3D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "def logistic_regresor(units=1,input_dim=32*32*3,\n",
    "\t\t\t\t\t\t activation='sigmoid', loss='binary_crossentropy',\n",
    "\t\t\t\t\t\t \t\t\t\toptimizer='sgd',metrics = 'accuracy'):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(units, input_dim=input_dim, activation=activation))\n",
    "\n",
    "\tmodel.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[metrics])\n",
    "\treturn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_model(input_shape=(3,90,150), lr =0.001, kernels=16, stride=(5,5),pool_size=(2,2), dense=50):\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Conv2D(kernels, stride, input_shape=input_shape,data_format=\"channels_first\"))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(dense, activation='relu'))\n",
    "\tmodel.add(Dense(2, activation='softmax'))\n",
    "\tmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=lr),\n",
    "              metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_model(input_shape=(3,100,100), lr =0.001, kernels=16, stride=(13,13),pool_size=(2,2), dense=50):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(kernels, stride, input_shape=input_shape))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=pool_size))\n",
    "\tmodel.add(Conv2D(kernels, stride, input_shape=input_shape))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=pool_size))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(dense, activation='relu'))\n",
    "\tmodel.add(Dense(2, activation='softmax'))\n",
    "\tmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=lr),\n",
    "              metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkpoints  \n",
    "\n",
    "Training Neuronal Networks  can take long and might envolve some errors or issues on the way. Is always a good practice, for instance, save the weights often, save the logs according to the metrics used, and stop early if the training is not improving learning.  Gladly for us,  Keras make it very simple!  The keras  callbacks are design to call out functions with some periodicity to check how the network is doing. Let's check some of them\n",
    "\n",
    "* ModelCheckpoint: \n",
    "\n",
    "This call backs let us save the weights in some specific location, according to the periodicity we want or the metric we are using. \n",
    "\n",
    "* CSVLogger: We can also save all the logs obtained in an indicated location in csv format. \n",
    "\n",
    "* TensorBoard: This tool creates also logs, but it also includes an interface for visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "WEIGHTS_BEST = 'weights/weights.best.h5'\n",
    "TRAINING_LOG = 'logs/training_log,csv'\n",
    "LOGS_DIR = './logs'\n",
    "checkpoint = ModelCheckpoint(WEIGHTS_BEST, monitor='loss', verbose=0, save_best_only=False, save_weights_only=True, mode='min', period=1) \n",
    "csv_logger = CSVLogger(TRAINING_LOG, append=True)\n",
    "tb = TensorBoard(log_dir=LOGS_DIR, histogram_freq=0, write_graph=True, write_images=False)\n",
    "callbacks_list = [ checkpoint, csv_logger, tb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your model, you can star training. To do so you can call the model.fit(*parameters*) method. \n",
    "\n",
    "Fit method, have the following inputs\n",
    "\n",
    "Data, target : Data for training and labels. More complicated models not only accept labels but could accept ground_truth images, such as heatmaps. Also this can be replaced by a generator that can be used for augmenting data. \n",
    "\n",
    "batch Size = How many images are you feeding on each step.\n",
    "\n",
    "epochs = The maximum number of epocs that you want to train your model. \n",
    "\n",
    "validation data= You might want to save the performance of your model while is training, so you can put your validation data to check the performance in your testing dataset. \n",
    "\n",
    "callbacks: see previous section\n",
    "\n",
    "After you finish training you can evaluate your model by providing testing dataset. You can specify what kind of metrics you can use such as F-1 score, accuracy, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_iter= 200 \n",
    "model=logistic_regresor()\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs=max_iter,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=callbacks_list)\n",
    "results = model.evaluate(self.test,self.test_y)\n",
    "H_logistic = pd.DataFrame(history.history, index=history.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One layer Shallow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_iter= 200 \n",
    "model=shallow_model()\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs=max_iter,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=callbacks_list)\n",
    "results = model.evaluate(self.test,self.test_y)\n",
    "H_logistic = pd.DataFrame(history.history, index=history.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer Shallow Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_iter= 200 \n",
    "model=two_layer_model()\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs=max_iter,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=callbacks_list)\n",
    "results = model.evaluate(self.test,self.test_y)\n",
    "H_logistic = pd.DataFrame(history.history, index=history.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writting the scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash File and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
